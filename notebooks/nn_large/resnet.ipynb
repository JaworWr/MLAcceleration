{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250de52c-2b98-4d36-9cf4-78a7a4263860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from nn_extrapolation import AcceleratedSGD\n",
    "from nn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bulgarian-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    device=\"cuda:0\",\n",
    "    loss_fn=nn.NLLLoss(reduction=\"mean\"),\n",
    "    val_loss_fn=nn.NLLLoss(reduction=\"sum\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "manual-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dl = load_dataset(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root=os.path.join(\"/tmp\", os.environ[\"USER\"], \"CIFAR\"),\n",
    "    validation_split=0.2,\n",
    "    batch_size=128,\n",
    "    num_workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "preliminary-ridge",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 10),\n",
    "    nn.LogSoftmax(-1)\n",
    ")\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b02c57-05ac-487e-92cf-1d639e4e41df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0994, 2.3962655769348142)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validation(model, dl[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-produce",
   "metadata": {},
   "source": [
    "## No momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impossible-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AcceleratedSGD(model.parameters(), 1e-1, k=5, momentum=0, weight_decay=1e-5)\n",
    "logger = Logger(\"resnet_log_no_momentum.txt.no_resizing\")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forty-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 20.14it/s, loss=1.6812]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 1 | Training loss: 1.6812, validation accuracy: 0.5009, validation loss: 1.4852\n",
      "100%|██████████| 313/313 [00:15<00:00, 19.75it/s, loss=1.1737]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 2 | Training loss: 1.1737, validation accuracy: 0.5765, validation loss: 1.2064\n",
      "100%|██████████| 313/313 [00:15<00:00, 19.94it/s, loss=0.9536]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 3 | Training loss: 0.9536, validation accuracy: 0.6255, validation loss: 1.0741\n",
      "100%|██████████| 313/313 [00:15<00:00, 19.65it/s, loss=0.7974]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 4 | Training loss: 0.7974, validation accuracy: 0.6062, validation loss: 1.1609\n",
      "100%|██████████| 313/313 [00:15<00:00, 19.78it/s, loss=0.6590]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 5 | Training loss: 0.6590, validation accuracy: 0.6603, validation loss: 1.0318\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.35it/s, loss=0.3482]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 6 | Training loss: 0.3482, validation accuracy: 0.7236, validation loss: 0.8742\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.40it/s, loss=0.2341]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 7 | Training loss: 0.2341, validation accuracy: 0.7213, validation loss: 0.9533\n",
      "100%|██████████| 313/313 [00:16<00:00, 18.61it/s, loss=0.1654]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 8 | Training loss: 0.1654, validation accuracy: 0.7202, validation loss: 1.0678\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.32it/s, loss=0.1082]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 9 | Training loss: 0.1082, validation accuracy: 0.7148, validation loss: 1.2011\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.39it/s, loss=0.0697]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10 | Training loss: 0.0697, validation accuracy: 0.7124, validation loss: 1.2990\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.31it/s, loss=0.0410]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 11 | Training loss: 0.0410, validation accuracy: 0.7142, validation loss: 1.3090\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.15it/s, loss=0.0376]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 | Training loss: 0.0376, validation accuracy: 0.7138, validation loss: 1.3161\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.19it/s, loss=0.0341]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 | Training loss: 0.0341, validation accuracy: 0.7148, validation loss: 1.3130\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.09it/s, loss=0.0324]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14 | Training loss: 0.0324, validation accuracy: 0.7161, validation loss: 1.3220\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.13it/s, loss=0.0301]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 15 | Training loss: 0.0301, validation accuracy: 0.7164, validation loss: 1.3159\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.13it/s, loss=0.0305]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16 | Training loss: 0.0305, validation accuracy: 0.7158, validation loss: 1.3387\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.13it/s, loss=0.0280]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 17 | Training loss: 0.0280, validation accuracy: 0.7164, validation loss: 1.3466\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.22it/s, loss=0.0267]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 18 | Training loss: 0.0267, validation accuracy: 0.7141, validation loss: 1.3513\n"
     ]
    }
   ],
   "source": [
    "epochs = 18\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = trainer.train_epoch(model, optimizer, dl[\"train\"])\n",
    "    scheduler.step()\n",
    "    optimizer.finish_epoch()\n",
    "    val_acc, val_loss = trainer.validation(model, dl[\"valid\"])\n",
    "    logger.log(\"Epoch\", epoch+1, \"|\", \n",
    "          f\"Training loss: {train_loss:.4f}, validation accuracy: {val_acc:.4f}, validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "breathing-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.999, 0.01205787510573864)\n",
      "Valid: (0.7166, 1.3761080833435058)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "experienced-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.accelerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entire-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.store_parameters()\n",
    "model.to(trainer.device)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mexican-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.998975, 0.01300976274833083)\n",
      "Valid: (0.7164, 1.36950100440979)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-writer",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demonstrated-posting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 10),\n",
    "    nn.LogSoftmax(-1)\n",
    ")\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advisory-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AcceleratedSGD(model.parameters(), 1e-1, k=5, momentum=0.9,  weight_decay=1e-5)\n",
    "logger = Logger(\"resnet_log_momentum.txt.no_resizing\")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d730147d-5f00-4691-8670-b5800f9bcaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 17.27it/s, loss=2.2050]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 1 | Training loss: 2.2050, validation accuracy: 0.4242, validation loss: 1.5878\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.91it/s, loss=1.4366]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 2 | Training loss: 1.4366, validation accuracy: 0.5195, validation loss: 1.3524\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.25it/s, loss=1.2166]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 3 | Training loss: 1.2166, validation accuracy: 0.5997, validation loss: 1.1192\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.97it/s, loss=1.0520]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 4 | Training loss: 1.0520, validation accuracy: 0.6385, validation loss: 1.0259\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.06it/s, loss=0.9105]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 5 | Training loss: 0.9105, validation accuracy: 0.6580, validation loss: 0.9670\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.86it/s, loss=0.6564]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 6 | Training loss: 0.6564, validation accuracy: 0.7117, validation loss: 0.8501\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.97it/s, loss=0.5825]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 7 | Training loss: 0.5825, validation accuracy: 0.7143, validation loss: 0.8368\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.73it/s, loss=0.5292]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 8 | Training loss: 0.5292, validation accuracy: 0.7207, validation loss: 0.8526\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.69it/s, loss=0.4808]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 9 | Training loss: 0.4808, validation accuracy: 0.7139, validation loss: 0.8823\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.58it/s, loss=0.4274]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10 | Training loss: 0.4274, validation accuracy: 0.7187, validation loss: 0.8958\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.48it/s, loss=0.3415]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 11 | Training loss: 0.3415, validation accuracy: 0.7212, validation loss: 0.9230\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.41it/s, loss=0.3215]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 | Training loss: 0.3215, validation accuracy: 0.7196, validation loss: 0.9609\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.44it/s, loss=0.3095]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 | Training loss: 0.3095, validation accuracy: 0.7247, validation loss: 0.9685\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.66it/s, loss=0.2986]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14 | Training loss: 0.2986, validation accuracy: 0.7233, validation loss: 0.9867\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.49it/s, loss=0.2899]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 15 | Training loss: 0.2899, validation accuracy: 0.7212, validation loss: 1.0164\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.44it/s, loss=0.2819]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16 | Training loss: 0.2819, validation accuracy: 0.7226, validation loss: 1.0176\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.61it/s, loss=0.2685]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 17 | Training loss: 0.2685, validation accuracy: 0.7196, validation loss: 1.0652\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.61it/s, loss=0.2605]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 18 | Training loss: 0.2605, validation accuracy: 0.7210, validation loss: 1.0698\n"
     ]
    }
   ],
   "source": [
    "epochs = 18\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = trainer.train_epoch(model, optimizer, dl[\"train\"])\n",
    "    scheduler.step()\n",
    "    optimizer.finish_epoch()\n",
    "    val_acc, val_loss = trainer.validation(model, dl[\"valid\"])\n",
    "    logger.log(\"Epoch\", epoch+1, \"|\", \n",
    "          f\"Training loss: {train_loss:.4f}, validation accuracy: {val_acc:.4f}, validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91932135-cf5f-4235-aeb2-33a483b6b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.93575, 0.2349222775220871)\n",
      "Valid: (0.721, 1.06977393951416)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487c63f6-17f9-46e0-8f65-8b2b50953333",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.accelerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89677e9c-16fa-4d0b-8692-092f808cc36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.store_parameters()\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e7e848-a0ef-46ae-bc0c-1dc52d823736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.9302, 0.2552979892492294)\n",
      "Valid: (0.7206, 1.079448751449585)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-solution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
