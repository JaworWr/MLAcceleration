{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compound-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from nn_extrapolation import AcceleratedSGD\n",
    "from nn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bulgarian-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    device=\"cuda:0\",\n",
    "    loss_fn=nn.NLLLoss(reduction=\"mean\"),\n",
    "    val_loss_fn=nn.NLLLoss(reduction=\"sum\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dl = load_dataset(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root=os.path.join(\"/tmp\", os.environ[\"USER\"], \"CIFAR\"),\n",
    "    validation_split=0.2,\n",
    "    batch_size=128,\n",
    "    num_workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preliminary-ridge",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 10),\n",
    "    nn.LogSoftmax(-1)\n",
    ")\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b02c57-05ac-487e-92cf-1d639e4e41df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0983, 2.3421695240020752)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validation(model, dl[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-produce",
   "metadata": {},
   "source": [
    "## No momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impossible-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AcceleratedSGD(model.parameters(), 1e-1, k=5, momentum=0, weight_decay=1e-5)\n",
    "logger = Logger(\"resnet_log_no_momentum.txt.no_resizing\")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forty-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:16<00:00, 19.33it/s, loss=1.7678]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 1 | Training loss: 1.7678, validation accuracy: 0.4688, validation loss: 1.5258\n",
      "100%|██████████| 313/313 [00:17<00:00, 18.12it/s, loss=1.2168]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 2 | Training loss: 1.2168, validation accuracy: 0.5295, validation loss: 1.4671\n",
      "100%|██████████| 313/313 [00:17<00:00, 17.53it/s, loss=0.9899]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 3 | Training loss: 0.9899, validation accuracy: 0.6141, validation loss: 1.1058\n",
      "100%|██████████| 313/313 [00:17<00:00, 17.51it/s, loss=0.8302]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 4 | Training loss: 0.8302, validation accuracy: 0.5912, validation loss: 1.2577\n",
      "100%|██████████| 313/313 [00:17<00:00, 17.47it/s, loss=0.6900]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 5 | Training loss: 0.6900, validation accuracy: 0.6536, validation loss: 1.0680\n",
      "100%|██████████| 313/313 [00:17<00:00, 17.48it/s, loss=0.3658]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 6 | Training loss: 0.3658, validation accuracy: 0.7155, validation loss: 0.9025\n",
      "100%|██████████| 313/313 [00:17<00:00, 17.65it/s, loss=0.2535]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 7 | Training loss: 0.2535, validation accuracy: 0.7111, validation loss: 1.0054\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.10it/s, loss=0.1837]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 8 | Training loss: 0.1837, validation accuracy: 0.7064, validation loss: 1.1135\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.34it/s, loss=0.1240]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 9 | Training loss: 0.1240, validation accuracy: 0.7062, validation loss: 1.2458\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.35it/s, loss=0.0774]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10 | Training loss: 0.0774, validation accuracy: 0.7030, validation loss: 1.3504\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.23it/s, loss=0.0467]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 11 | Training loss: 0.0467, validation accuracy: 0.7044, validation loss: 1.3542\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.29it/s, loss=0.0424]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 | Training loss: 0.0424, validation accuracy: 0.7037, validation loss: 1.3853\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.23it/s, loss=0.0390]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 | Training loss: 0.0390, validation accuracy: 0.7037, validation loss: 1.3832\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.14it/s, loss=0.0367]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14 | Training loss: 0.0367, validation accuracy: 0.7049, validation loss: 1.3907\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.26it/s, loss=0.0339]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 15 | Training loss: 0.0339, validation accuracy: 0.7048, validation loss: 1.3952\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.21it/s, loss=0.0321]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16 | Training loss: 0.0321, validation accuracy: 0.7051, validation loss: 1.4105\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.14it/s, loss=0.0305]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 17 | Training loss: 0.0305, validation accuracy: 0.7035, validation loss: 1.4122\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.31it/s, loss=0.0294]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 18 | Training loss: 0.0294, validation accuracy: 0.7035, validation loss: 1.4423\n"
     ]
    }
   ],
   "source": [
    "epochs = 18\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = trainer.train_epoch(model, optimizer, dl[\"train\"])\n",
    "    scheduler.step()\n",
    "    optimizer.finish_epoch()\n",
    "    val_acc, val_loss = trainer.validation(model, dl[\"valid\"])\n",
    "    logger.log(\"Epoch\", epoch+1, \"|\", \n",
    "          f\"Training loss: {train_loss:.4f}, validation accuracy: {val_acc:.4f}, validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "breathing-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.99905, 0.013996499344706535)\n",
      "Valid: (0.7035, 1.44233853931427)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "experienced-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.accelerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entire-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.store_parameters()\n",
    "model.to(trainer.device)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mexican-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.998825, 0.01517753880470991)\n",
      "Valid: (0.7039, 1.4358053766250611)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-writer",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demonstrated-posting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 10),\n",
    "    nn.LogSoftmax(-1)\n",
    ")\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advisory-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AcceleratedSGD(model.parameters(), 1e-1, k=5, momentum=0.9,  weight_decay=1e-5)\n",
    "logger = Logger(\"resnet_log_momentum.txt.no_resizing\")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d730147d-5f00-4691-8670-b5800f9bcaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 17.27it/s, loss=2.2050]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 1 | Training loss: 2.2050, validation accuracy: 0.4242, validation loss: 1.5878\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.91it/s, loss=1.4366]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 2 | Training loss: 1.4366, validation accuracy: 0.5195, validation loss: 1.3524\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.25it/s, loss=1.2166]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 3 | Training loss: 1.2166, validation accuracy: 0.5997, validation loss: 1.1192\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.97it/s, loss=1.0520]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 4 | Training loss: 1.0520, validation accuracy: 0.6385, validation loss: 1.0259\n",
      "100%|██████████| 313/313 [00:18<00:00, 17.06it/s, loss=0.9105]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 5 | Training loss: 0.9105, validation accuracy: 0.6580, validation loss: 0.9670\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.86it/s, loss=0.6564]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 6 | Training loss: 0.6564, validation accuracy: 0.7117, validation loss: 0.8501\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.97it/s, loss=0.5825]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 7 | Training loss: 0.5825, validation accuracy: 0.7143, validation loss: 0.8368\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.73it/s, loss=0.5292]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 8 | Training loss: 0.5292, validation accuracy: 0.7207, validation loss: 0.8526\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.69it/s, loss=0.4808]\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 9 | Training loss: 0.4808, validation accuracy: 0.7139, validation loss: 0.8823\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.58it/s, loss=0.4274]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 10 | Training loss: 0.4274, validation accuracy: 0.7187, validation loss: 0.8958\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.48it/s, loss=0.3415]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 11 | Training loss: 0.3415, validation accuracy: 0.7212, validation loss: 0.9230\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.41it/s, loss=0.3215]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 | Training loss: 0.3215, validation accuracy: 0.7196, validation loss: 0.9609\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.44it/s, loss=0.3095]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 | Training loss: 0.3095, validation accuracy: 0.7247, validation loss: 0.9685\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.66it/s, loss=0.2986]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14 | Training loss: 0.2986, validation accuracy: 0.7233, validation loss: 0.9867\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.49it/s, loss=0.2899]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 15 | Training loss: 0.2899, validation accuracy: 0.7212, validation loss: 1.0164\n",
      "100%|██████████| 313/313 [00:19<00:00, 16.44it/s, loss=0.2819]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16 | Training loss: 0.2819, validation accuracy: 0.7226, validation loss: 1.0176\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.61it/s, loss=0.2685]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 17 | Training loss: 0.2685, validation accuracy: 0.7196, validation loss: 1.0652\n",
      "100%|██████████| 313/313 [00:18<00:00, 16.61it/s, loss=0.2605]\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 18 | Training loss: 0.2605, validation accuracy: 0.7210, validation loss: 1.0698\n"
     ]
    }
   ],
   "source": [
    "epochs = 18\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = trainer.train_epoch(model, optimizer, dl[\"train\"])\n",
    "    scheduler.step()\n",
    "    optimizer.finish_epoch()\n",
    "    val_acc, val_loss = trainer.validation(model, dl[\"valid\"])\n",
    "    logger.log(\"Epoch\", epoch+1, \"|\", \n",
    "          f\"Training loss: {train_loss:.4f}, validation accuracy: {val_acc:.4f}, validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91932135-cf5f-4235-aeb2-33a483b6b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.93575, 0.2349222775220871)\n",
      "Valid: (0.721, 1.06977393951416)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487c63f6-17f9-46e0-8f65-8b2b50953333",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.accelerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89677e9c-16fa-4d0b-8692-092f808cc36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.store_parameters()\n",
    "model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e7e848-a0ef-46ae-bc0c-1dc52d823736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.9302, 0.2552979892492294)\n",
      "Valid: (0.7206, 1.079448751449585)\n"
     ]
    }
   ],
   "source": [
    "train_score = trainer.validation(model, dl[\"train\"])\n",
    "valid_score = trainer.validation(model, dl[\"valid\"])\n",
    "logger.log(\"Train:\", train_score)\n",
    "logger.log(\"Valid:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-solution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
